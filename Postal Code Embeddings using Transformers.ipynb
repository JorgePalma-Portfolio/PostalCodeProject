{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6661cf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "## This expands a notebook to full width\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "display(HTML(\"\"\"<style>div.output_area{max-height:10000px;overflow:scroll;}</style>\"\"\"))\n",
    "## Show Python Version\n",
    "import sys\n",
    "print(\"Python: {0}\".format(sys.version))\n",
    "\n",
    "## Show Current Time\n",
    "import datetime as dt\n",
    "start = dt.datetime.now()\n",
    "print(\"Notebook Last Run Initiated: \"+str(start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8108e7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf53b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 23\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 512\n",
    "num_epochs = 50\n",
    "projection_dim = 60\n",
    "num_heads = 4\n",
    "\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "\n",
    "transformer_layers = 4\n",
    "\n",
    "embed_dim = 60\n",
    "embeddings_shape = (1,embed_dim)\n",
    "\n",
    "mlp_head_units = [1024, 512]  # Size of the dense layers of the final classifier\n",
    "\n",
    "ckp_path = 'models/Model_Embedding_transformers.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0bf8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5651a552",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super().__init__()\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        return positions + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e09e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingFixedWeights(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, output_dim, **kwargs):\n",
    "        super(PositionEmbeddingFixedWeights, self).__init__(**kwargs)\n",
    "        word_embedding_matrix = self.get_position_encoding(vocab_size, output_dim)   \n",
    "        position_embedding_matrix = self.get_position_encoding(sequence_length, output_dim)                                          \n",
    "        self.word_embedding_layer = Embedding(\n",
    "            input_dim=vocab_size, output_dim=output_dim,\n",
    "            weights=[word_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "        self.position_embedding_layer = Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim,\n",
    "            weights=[position_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "             \n",
    "    def get_position_encoding(self, seq_len, d, n=10000):\n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(d/2)):\n",
    "                denominator = np.power(n, 2*i/d)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i+1] = np.cos(k/denominator)\n",
    "        return P\n",
    "\n",
    "\n",
    "    def call(self, inputs):        \n",
    "        position_indices = tf.range(tf.shape(inputs)[1])\n",
    "        embedded_words = self.word_embedding_layer(inputs)\n",
    "        embedded_indices = self.position_embedding_layer(position_indices)\n",
    "        return embedded_words + embedded_indices"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1368444b",
   "metadata": {},
   "source": [
    "attnisallyouneed_embedding = PositionEmbeddingFixedWeights(output_sequence_length,vocab_size, output_length)\n",
    "attnisallyouneed_output = attnisallyouneed_embedding(vectorized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1440f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453c1cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(learning_rate):\n",
    "    \n",
    "    input_text  = layers.Input(shape=(max_length-1,),dtype=\"int32\",name='input_text')\n",
    "    \n",
    "    embeddings = tf.keras.layers.Embedding(input_dim=vocab_size, input_length=max_length, output_dim=embed_dim,name='embeddings')(input_text)\n",
    "    \n",
    "    embeddings = PositionEmbedding(maxlen=max_length-1,embed_dim=embed_dim)(embeddings)\n",
    "    \n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(embeddings)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1)(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, embeddings])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        embeddings = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(embeddings)\n",
    "    representation = layers.GlobalAveragePooling1D()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    \n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    \n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(num_classes,activation='softmax', name='activation')(features)\n",
    "    \n",
    "    # Create the Keras model.\n",
    "    model = Model(inputs=input_text, outputs=outputs,name='Postal_Code_Embeddings')\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate),loss=\"sparse_categorical_crossentropy\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c52b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input vector, returns nearest word(s)\n",
    "def Cosine_Similarity(word,weight,word_to_index,vocab_size,index_to_word):\n",
    "    \n",
    "    #Get the index of the word from the dictionary\n",
    "    index = word_to_index[word]\n",
    "    \n",
    "    #Get the correspondin weights for the word\n",
    "    word_vector_1 = weight[index]\n",
    "    \n",
    "    word_similarity = {}\n",
    "\n",
    "    for i in range(vocab_size):\n",
    "        \n",
    "        j = i\n",
    "        \n",
    "        word_vector_2 = weight[j]\n",
    "        \n",
    "        theta_sum = np.dot(word_vector_1, word_vector_2)\n",
    "        theta_den = np.linalg.norm(word_vector_1) * np.linalg.norm(word_vector_2)\n",
    "        theta = theta_sum / theta_den\n",
    "        \n",
    "        word = index_to_word[j]\n",
    "        word_similarity[word] = theta\n",
    "    \n",
    "    return word_similarity #words_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655a7376",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define documents\n",
    "data = pd.read_csv('data\\zipcodedata.csv')\n",
    "docs = data[['data']].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6f907f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tokenizer\n",
    "t = Tokenizer(filters=' ')\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f6cde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode the documents\n",
    "vocab_size = len(t.word_index) + 1\n",
    "num_classes = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969994d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_docs = t.texts_to_sequences(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389e7801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad documents to a max length of 3 words, windows size = 3\n",
    "max_length = 3\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c345f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(padded_docs[:,[0,1]])\n",
    "label = padded_docs[:,[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff34bf44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = create_classifier(learning_rate)\n",
    "        \n",
    "lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'loss', factor = 0.4, patience = 2, verbose = 0, min_delta = 0.001, mode = 'min')\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='min', verbose=0, patience=5, restore_best_weights=True)\n",
    "mc = tf.keras.callbacks.ModelCheckpoint(ckp_path, monitor='loss', mode='min', verbose=0, save_best_only=True, save_weights_only=True)\n",
    "        \n",
    "# train the model\n",
    "history = model.fit(x=train,\n",
    "                    y=label,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    callbacks=[mc,lr,es],  \n",
    "                    shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd29386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_classifier(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446cf98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(ckp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ce366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = model.get_layer('embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7ea21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embedding_layer.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69b3475",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"embeddings_model_w2v.csv\", embeddings, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156326f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = t.word_index\n",
    "index_to_word = dict()\n",
    "\n",
    "for key in word_to_index:\n",
    "    index_to_word.update({word_to_index[key] : key })\n",
    "\n",
    "word_to_index.update({'unk':0})\n",
    "index_to_word.update({0:'unk'})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d889cf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[t.word_index['illinois']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f3039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(embeddings[t.word_index['illinois']].reshape(1,60),embeddings[t.word_index['oregon']].reshape(1,60))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08750361",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(embeddings[t.word_index['60126']].reshape(1,60),embeddings[t.word_index['60181']].reshape(1,60))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d66f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "il_60126 = np.reshape(embeddings[t.word_index['illinois']],embeddings_shape) + np.reshape(embeddings[t.word_index['60126']],embeddings_shape)\n",
    "il_60181 = np.reshape(embeddings[t.word_index['illinois']],embeddings_shape) + np.reshape(embeddings[t.word_index['60181']],embeddings_shape)\n",
    "or_97035 = np.reshape(embeddings[t.word_index['oregon']],embeddings_shape) + np.reshape(embeddings[t.word_index['97035']],embeddings_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31f6099",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(il_60126,il_60181)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04167c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(il_60126,or_97035)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4767cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "il_60126 += np.reshape(embeddings[t.word_index['dupage'],:],embeddings_shape)\n",
    "il_60181 += np.reshape(embeddings[t.word_index['dupage'],:],embeddings_shape)\n",
    "or_97035 += np.reshape(embeddings[t.word_index['clackamas'],:],embeddings_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d0a42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(il_60126,il_60181)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe9379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(il_60126,or_97035)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4402bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define documents\n",
    "data = pd.read_csv('data\\ZIP2LATLON_VER1.csv',dtype={\n",
    "                   'postal_code': str,\n",
    "                   'country_code': str,\n",
    "                   'place': str,\n",
    "                   'state': str,\n",
    "                   'statecode': str,\n",
    "                   'province_or_county': str,\n",
    "                   'province_or_countycode': str,\n",
    "                   'latitude': float,\n",
    "                   'longitude': float})\n",
    "\n",
    "data['postal_code'] = data['postal_code'].str.zfill(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c6c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = pd.DataFrame(pd.unique(data.statecode),columns=['states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e086956a",
   "metadata": {},
   "outputs": [],
   "source": [
    "states_list = []\n",
    "\n",
    "for index, row in states.iterrows():\n",
    "    state = row[0].lower()\n",
    "    states_list.append(pd.concat([pd.Series(state), pd.DataFrame(np.reshape(embeddings[t.word_index[state],:],embeddings_shape))   ],axis=1))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4881ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = pd.concat(states_list)\n",
    "states = states.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6ef199",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "states.columns = range(states.columns.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11494cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = StandardScaler().fit_transform(states.iloc[:,1:-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64311d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TSNE : Compressing the weights to 3 dimensions to plot the data\n",
    "tsne_model = TSNE(perplexity=40, n_components=3, init='pca', n_iter=2500, random_state=seed)\n",
    "new_values = tsne_model.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b322485",
   "metadata": {},
   "outputs": [],
   "source": [
    "principalDf = pd.DataFrame(data = new_values, columns = ['component 1', 'component 2', 'component 3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d55113",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf = pd.concat([principalDf, states.iloc[:,0]], axis = 1)\n",
    "finalDf.columns = ['x', 'y', 'z', 'State']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002db0b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_3d(finalDf, x='x', y='y', z='z',color='State')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c0ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "postalcodes = pd.DataFrame(pd.unique(data.postal_code),columns=['postal_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3c4b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "postalcodes_list = []\n",
    "\n",
    "for index, row in postalcodes.iterrows():\n",
    "    postalcode = row[0].lower()\n",
    "    postalcodes_list.append(pd.concat([pd.Series(postalcode), pd.DataFrame(np.reshape(embeddings[t.word_index[postalcode],:],(1,embed_dim)))   ],axis=1))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debdbf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "postalcodes = pd.concat(postalcodes_list)\n",
    "postalcodes = postalcodes.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8930b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "postalcodes.columns = range(postalcodes.columns.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a4d2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = StandardScaler().fit_transform(postalcodes.iloc[:,1:-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e534af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TSNE : Compressing the weights to 3 dimensions to plot the data\n",
    "tsne_model = TSNE(perplexity=40, n_components=3, init='pca', n_iter=2500, random_state=23)\n",
    "new_values = tsne_model.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc775c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "principalDf = pd.DataFrame(data = new_values, columns = ['component 1', 'component 2', 'component 3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b50391",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf = pd.concat([principalDf, postalcodes.iloc[:,0]], axis = 1)\n",
    "finalDf.columns = ['x', 'y', 'z', 'postalcode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764a2085",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf = finalDf[finalDf['postalcode'].str.slice(start=0, stop=3) == '601']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2112a1bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_3d(finalDf, x='x', y='y', z='z',color='postalcode')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec73710",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
