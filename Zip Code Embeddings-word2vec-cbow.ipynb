{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6661cf68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>div.output_area{max-height:10000px;overflow:scroll;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.8.8 (default, Feb 24 2021, 15:54:32) [MSC v.1928 64 bit (AMD64)]\n",
      "Notebook Last Run Initiated: 2022-06-16 06:17:35.299261\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "## This expands a notebook to full width\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "display(HTML(\"\"\"<style>div.output_area{max-height:10000px;overflow:scroll;}</style>\"\"\"))\n",
    "## Show Python Version\n",
    "import sys\n",
    "print(\"Python: {0}\".format(sys.version))\n",
    "\n",
    "## Show Current Time\n",
    "import datetime as dt\n",
    "start = dt.datetime.now()\n",
    "print(\"Notebook Last Run Initiated: \"+str(start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8108e7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "seed = 23\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "\n",
    "from tensorflow.compat.v1 import set_random_seed \n",
    "set_random_seed(seed)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc687efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "        \n",
    "    def __init__(self, df_X, seq_Y, batch_size=32, vocab_size=None, shuffle=False):\n",
    "        self.batch_size = batch_size\n",
    "        self.df_X = df_X\n",
    "        self.seq_Y = seq_Y\n",
    "        self.indices = self.df_X.index.tolist()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.indices) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.index[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch = [self.indices[k] for k in index]\n",
    "        \n",
    "        X, y = self.__get_data(batch)\n",
    "        return X, y\n",
    "    \n",
    "    def n(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.index = np.arange(len(self.indices))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.index)\n",
    "\n",
    "    def __get_data(self, batch):\n",
    "        X1 = []\n",
    "        y  = []\n",
    "        \n",
    "        for i, id in enumerate(batch):\n",
    "            \n",
    "            # Data\n",
    "            docs = self.df_X.iloc[self.indices[id]]\n",
    "                       \n",
    "            #Labels\n",
    "            output_seq = self.seq_Y[ self.indices[id]]\n",
    "            output_seq = to_categorical([output_seq], num_classes=self.vocab_size)[0]\n",
    "                \n",
    "            X1.append(docs)\n",
    "            y.append(output_seq)\n",
    "                            \n",
    "        return np.array(X1), np.array(y).reshape(self.batch_size,self.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01c52b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input vector, returns nearest word(s)\n",
    "def Cosine_Similarity(word,weight,word_to_index,vocab_size,index_to_word):\n",
    "    \n",
    "    #Get the index of the word from the dictionary\n",
    "    index = word_to_index[word]\n",
    "    \n",
    "    #Get the correspondin weights for the word\n",
    "    word_vector_1 = weight[index]\n",
    "    \n",
    "    \n",
    "    word_similarity = {}\n",
    "\n",
    "    for i in range(vocab_size):\n",
    "        \n",
    "        j = i\n",
    "        \n",
    "        word_vector_2 = weight[j]\n",
    "        \n",
    "        theta_sum = np.dot(word_vector_1, word_vector_2)\n",
    "        theta_den = np.linalg.norm(word_vector_1) * np.linalg.norm(word_vector_2)\n",
    "        theta = theta_sum / theta_den\n",
    "        \n",
    "        word = index_to_word[j]\n",
    "        word_similarity[word] = theta\n",
    "    \n",
    "    return word_similarity #words_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3487f460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildModel(learning_rate):\n",
    "        \n",
    "    input_text  = tf.keras.layers.Input(shape=(max_length-1,),dtype=\"int32\",name='text')\n",
    "\n",
    "    x = tf.keras.layers.Embedding(input_dim=vocab_size, input_length=max_length, output_dim=20,name='embeddings')(input_text)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(400, activation='relu',name='act01')(x)\n",
    "    \n",
    "    output = tf.keras.layers.Dense(vocab_size, activation='softmax', name='act02')(x)\n",
    "    \n",
    "    model = Model(input_text,output)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate),loss=\"categorical_crossentropy\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "655a7376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define documents\n",
    "data = pd.read_csv('data\\zipcodedata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd6cc4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = data.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e6f907f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6f6cde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode the documents\n",
    "vocab_size = len(t.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "969994d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_docs = t.texts_to_sequences(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "389e7801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad documents to a max length of 3 words, windows size = 3\n",
    "max_length = 3\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20c345f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(padded_docs[:,[0,1]])\n",
    "label = padded_docs[:,[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a671fe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "train_generator = DataGenerator(df_X=train, seq_Y=label, batch_size=batch_size, vocab_size=vocab_size, shuffle=True)\n",
    "STEP_SIZE_TRAIN=train_generator.n()//train_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff34bf44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/512\n",
      "9801/9801 [==============================] - 547s 56ms/step - loss: 5.4625 - lr: 0.0010\n",
      "Epoch 2/512\n",
      "9801/9801 [==============================] - 515s 53ms/step - loss: 3.5359 - lr: 0.0010\n",
      "Epoch 3/512\n",
      "9801/9801 [==============================] - 504s 51ms/step - loss: 2.9657 - lr: 0.0010\n",
      "Epoch 4/512\n",
      "9801/9801 [==============================] - 508s 52ms/step - loss: 2.7757 - lr: 0.0010\n",
      "Epoch 5/512\n",
      "9801/9801 [==============================] - 500s 51ms/step - loss: 2.6835 - lr: 0.0010\n",
      "Epoch 6/512\n",
      "9801/9801 [==============================] - 513s 52ms/step - loss: 2.6264 - lr: 0.0010\n",
      "Epoch 7/512\n",
      "9801/9801 [==============================] - 511s 52ms/step - loss: 2.5856 - lr: 0.0010\n",
      "Epoch 8/512\n",
      "9801/9801 [==============================] - 501s 51ms/step - loss: 2.5543 - lr: 0.0010\n",
      "Epoch 9/512\n",
      "9801/9801 [==============================] - 494s 50ms/step - loss: 2.5288 - lr: 0.0010\n",
      "Epoch 10/512\n",
      "9801/9801 [==============================] - 504s 51ms/step - loss: 2.5065 - lr: 0.0010\n",
      "Epoch 11/512\n",
      "9801/9801 [==============================] - 508s 52ms/step - loss: 2.4872 - lr: 0.0010\n",
      "Epoch 12/512\n",
      "9801/9801 [==============================] - 505s 52ms/step - loss: 2.4704 - lr: 0.0010\n",
      "Epoch 13/512\n",
      "9801/9801 [==============================] - 507s 52ms/step - loss: 2.4559 - lr: 0.0010\n",
      "Epoch 14/512\n",
      "9801/9801 [==============================] - 506s 52ms/step - loss: 2.4431 - lr: 0.0010\n",
      "Epoch 15/512\n",
      "9801/9801 [==============================] - 515s 53ms/step - loss: 2.4323 - lr: 0.0010\n",
      "Epoch 16/512\n",
      "9801/9801 [==============================] - 503s 51ms/step - loss: 2.4226 - lr: 0.0010\n",
      "Epoch 17/512\n",
      "9801/9801 [==============================] - 507s 52ms/step - loss: 2.4144 - lr: 0.0010\n",
      "Epoch 18/512\n",
      "9801/9801 [==============================] - 513s 52ms/step - loss: 2.4068 - lr: 0.0010\n",
      "Epoch 19/512\n",
      "9801/9801 [==============================] - 503s 51ms/step - loss: 2.4006 - lr: 0.0010\n",
      "Epoch 20/512\n",
      "9801/9801 [==============================] - 510s 52ms/step - loss: 2.3947 - lr: 0.0010\n",
      "Epoch 21/512\n",
      "9801/9801 [==============================] - 503s 51ms/step - loss: 2.3898 - lr: 0.0010\n",
      "Epoch 22/512\n",
      "9801/9801 [==============================] - 506s 52ms/step - loss: 2.3850 - lr: 0.0010\n",
      "Epoch 23/512\n",
      "9801/9801 [==============================] - 505s 52ms/step - loss: 2.3810 - lr: 0.0010\n",
      "Epoch 24/512\n",
      "9801/9801 [==============================] - 501s 51ms/step - loss: 2.3773 - lr: 0.0010\n",
      "Epoch 25/512\n",
      "9801/9801 [==============================] - 511s 52ms/step - loss: 2.3740 - lr: 0.0010\n",
      "Epoch 26/512\n",
      "9801/9801 [==============================] - 513s 52ms/step - loss: 2.3712 - lr: 0.0010\n",
      "Epoch 27/512\n",
      "9801/9801 [==============================] - 513s 52ms/step - loss: 2.3685 - lr: 0.0010\n",
      "Epoch 28/512\n",
      "9801/9801 [==============================] - 510s 52ms/step - loss: 2.3664 - lr: 0.0010\n",
      "Epoch 29/512\n",
      "9801/9801 [==============================] - 515s 53ms/step - loss: 2.3642 - lr: 0.0010\n",
      "Epoch 30/512\n",
      "9801/9801 [==============================] - 513s 52ms/step - loss: 2.3619 - lr: 0.0010\n",
      "Epoch 31/512\n",
      "9801/9801 [==============================] - 508s 52ms/step - loss: 2.3603 - lr: 0.0010\n",
      "Epoch 32/512\n",
      "9801/9801 [==============================] - 518s 53ms/step - loss: 2.3590 - lr: 0.0010\n",
      "Epoch 33/512\n",
      "9801/9801 [==============================] - 523s 53ms/step - loss: 2.3576 - lr: 0.0010\n",
      "Epoch 34/512\n",
      "9801/9801 [==============================] - 518s 53ms/step - loss: 2.3563 - lr: 0.0010\n",
      "Epoch 35/512\n",
      "9801/9801 [==============================] - 514s 52ms/step - loss: 2.3554 - lr: 0.0010\n",
      "Epoch 36/512\n",
      "9801/9801 [==============================] - 524s 53ms/step - loss: 2.3544 - lr: 0.0010\n",
      "Epoch 37/512\n",
      "9801/9801 [==============================] - 509s 52ms/step - loss: 2.3533 - lr: 0.0010\n",
      "Epoch 38/512\n",
      "9801/9801 [==============================] - 519s 53ms/step - loss: 2.3528 - lr: 0.0010\n",
      "Epoch 39/512\n",
      "9801/9801 [==============================] - 512s 52ms/step - loss: 2.3522 - lr: 0.0010\n",
      "Epoch 40/512\n",
      "9801/9801 [==============================] - 519s 53ms/step - loss: 2.3514 - lr: 0.0010\n",
      "Epoch 41/512\n",
      "9801/9801 [==============================] - 531s 54ms/step - loss: 2.3510 - lr: 0.0010\n",
      "Epoch 42/512\n",
      "9801/9801 [==============================] - 533s 54ms/step - loss: 2.3510 - lr: 0.0010\n",
      "Epoch 43/512\n",
      "9801/9801 [==============================] - 513s 52ms/step - loss: 2.3505 - lr: 0.0010\n",
      "Epoch 44/512\n",
      "9801/9801 [==============================] - 518s 53ms/step - loss: 2.2598 - lr: 4.0000e-04\n",
      "Epoch 45/512\n",
      "9801/9801 [==============================] - 533s 54ms/step - loss: 2.2487 - lr: 4.0000e-04\n",
      "Epoch 46/512\n",
      "9801/9801 [==============================] - 521s 53ms/step - loss: 2.2463 - lr: 4.0000e-04\n",
      "Epoch 47/512\n",
      "9801/9801 [==============================] - 515s 53ms/step - loss: 2.2452 - lr: 4.0000e-04\n",
      "Epoch 48/512\n",
      "9801/9801 [==============================] - 557s 57ms/step - loss: 2.2449 - lr: 4.0000e-04\n",
      "Epoch 49/512\n",
      "9801/9801 [==============================] - 530s 54ms/step - loss: 2.2443 - lr: 4.0000e-04\n",
      "Epoch 50/512\n",
      "9801/9801 [==============================] - 526s 54ms/step - loss: 2.2017 - lr: 1.6000e-04\n",
      "Epoch 51/512\n",
      "9801/9801 [==============================] - 525s 54ms/step - loss: 2.1979 - lr: 1.6000e-04\n",
      "Epoch 52/512\n",
      "9801/9801 [==============================] - 533s 54ms/step - loss: 2.1969 - lr: 1.6000e-04\n",
      "Epoch 53/512\n",
      "9801/9801 [==============================] - 528s 54ms/step - loss: 2.1963 - lr: 1.6000e-04\n",
      "Epoch 54/512\n",
      "9801/9801 [==============================] - 516s 53ms/step - loss: 2.1959 - lr: 1.6000e-04\n",
      "Epoch 55/512\n",
      "9801/9801 [==============================] - 532s 54ms/step - loss: 2.1764 - lr: 6.4000e-05\n",
      "Epoch 56/512\n",
      "9801/9801 [==============================] - 535s 55ms/step - loss: 2.1752 - lr: 6.4000e-05\n",
      "Epoch 57/512\n",
      "9801/9801 [==============================] - 522s 53ms/step - loss: 2.1748 - lr: 6.4000e-05\n",
      "Epoch 58/512\n",
      "9801/9801 [==============================] - 517s 53ms/step - loss: 2.1745 - lr: 6.4000e-05\n",
      "Epoch 59/512\n",
      "9801/9801 [==============================] - 524s 53ms/step - loss: 2.1659 - lr: 2.5600e-05\n",
      "Epoch 60/512\n",
      "9801/9801 [==============================] - 532s 54ms/step - loss: 2.1654 - lr: 2.5600e-05\n",
      "Epoch 61/512\n",
      "9801/9801 [==============================] - 520s 53ms/step - loss: 2.1653 - lr: 2.5600e-05\n",
      "Epoch 62/512\n",
      "9801/9801 [==============================] - 527s 54ms/step - loss: 2.1615 - lr: 1.0240e-05\n",
      "Epoch 63/512\n",
      "9801/9801 [==============================] - 531s 54ms/step - loss: 2.1613 - lr: 1.0240e-05\n",
      "Epoch 64/512\n",
      "9801/9801 [==============================] - 528s 54ms/step - loss: 2.1612 - lr: 1.0240e-05\n",
      "Epoch 65/512\n",
      "9801/9801 [==============================] - 522s 53ms/step - loss: 2.1596 - lr: 4.0960e-06\n",
      "Epoch 66/512\n",
      "9801/9801 [==============================] - 524s 53ms/step - loss: 2.1595 - lr: 4.0960e-06\n",
      "Epoch 67/512\n",
      "9801/9801 [==============================] - 529s 54ms/step - loss: 2.1594 - lr: 4.0960e-06\n",
      "Epoch 68/512\n",
      "9801/9801 [==============================] - 541s 55ms/step - loss: 2.1587 - lr: 1.6384e-06\n",
      "Epoch 69/512\n",
      "9801/9801 [==============================] - 528s 54ms/step - loss: 2.1587 - lr: 1.6384e-06\n",
      "Epoch 70/512\n",
      "9801/9801 [==============================] - 529s 54ms/step - loss: 2.1584 - lr: 6.5536e-07\n",
      "Epoch 71/512\n",
      "9801/9801 [==============================] - 546s 56ms/step - loss: 2.1584 - lr: 6.5536e-07\n",
      "Epoch 72/512\n",
      "9801/9801 [==============================] - 545s 56ms/step - loss: 2.1583 - lr: 6.5536e-07\n",
      "Epoch 73/512\n",
      "9801/9801 [==============================] - 539s 55ms/step - loss: 2.1582 - lr: 2.6214e-07\n",
      "Epoch 74/512\n",
      "9801/9801 [==============================] - 533s 54ms/step - loss: 2.1582 - lr: 2.6214e-07\n",
      "Epoch 75/512\n",
      "9801/9801 [==============================] - 536s 55ms/step - loss: 2.1582 - lr: 1.0486e-07\n",
      "Epoch 76/512\n",
      "9801/9801 [==============================] - 555s 57ms/step - loss: 2.1582 - lr: 1.0486e-07\n",
      "Epoch 77/512\n",
      "9801/9801 [==============================] - 547s 56ms/step - loss: 2.1582 - lr: 4.1943e-08\n",
      "Epoch 78/512\n",
      "9801/9801 [==============================] - 562s 57ms/step - loss: 2.1582 - lr: 4.1943e-08\n",
      "Epoch 79/512\n",
      "9801/9801 [==============================] - 535s 55ms/step - loss: 2.1582 - lr: 1.6777e-08\n",
      "Epoch 80/512\n",
      "9801/9801 [==============================] - 545s 56ms/step - loss: 2.1581 - lr: 1.6777e-08\n",
      "Epoch 81/512\n",
      "9801/9801 [==============================] - 539s 55ms/step - loss: 2.1582 - lr: 6.7109e-09\n",
      "Epoch 82/512\n",
      "9801/9801 [==============================] - 540s 55ms/step - loss: 2.1582 - lr: 6.7109e-09\n",
      "Epoch 83/512\n",
      "9801/9801 [==============================] - 542s 55ms/step - loss: 2.1582 - lr: 2.6844e-09\n",
      "Epoch 84/512\n",
      "9801/9801 [==============================] - 533s 54ms/step - loss: 2.1582 - lr: 2.6844e-09\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "n_epochs = 512\n",
    "\n",
    "model = BuildModel(learning_rate)\n",
    "        \n",
    "ckp_path = 'models/Model_w2v.hdf5'\n",
    "\n",
    "cb_lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'loss', factor = 0.4, patience = 2, verbose = 0, min_delta = 0.001, mode = 'min')\n",
    "        \n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='min', verbose=0, patience=4, restore_best_weights=True)\n",
    "mc = tf.keras.callbacks.ModelCheckpoint(ckp_path, monitor='loss', mode='min', verbose=0, save_best_only=True, save_weights_only=True)\n",
    "        \n",
    "    #tb_cb = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "\n",
    "# train the model\n",
    "history = model.fit(x=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    epochs=n_epochs,\n",
    "                    callbacks=[mc,cb_lr_schedule,es],  \n",
    "                    shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd29386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "model = BuildModel(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb1bd3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text (InputLayer)           [(None, 2)]               0         \n",
      "                                                                 \n",
      " embeddings (Embedding)      (None, 2, 20)             1129360   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 40)                0         \n",
      "                                                                 \n",
      " act01 (Dense)               (None, 400)               16400     \n",
      "                                                                 \n",
      " act02 (Dense)               (None, 56468)             22643668  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,789,428\n",
      "Trainable params: 23,789,428\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "446cf98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('models/Model_w2v.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3985bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 - 2s - loss: 1.9769 - 2s/epoch - 453ms/step\n",
      "1.976943850517273\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss = model.evaluate(train.iloc[0:100,:], to_categorical(label[0:100], num_classes=vocab_size), verbose=2)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90ce366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = model.get_layer('embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c7ea21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embedding_layer.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e69b3475",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"embeddings_model_w2v.csv\", embeddings, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17206d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.26666126,  0.303636  ,  1.0568444 , -0.03580691,  0.49443388,\n",
       "       -1.1844802 , -0.8280229 , -0.24679595,  1.3320878 , -1.2740679 ,\n",
       "        0.04827994,  1.3138323 , -0.6633513 ,  0.8621474 ,  0.25619456,\n",
       "        0.4419295 ,  0.06416243,  0.18444656, -0.3404408 , -0.23613566],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[t.word_index['60126'],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "77c611b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.056522503"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(np.reshape(embeddings[t.word_index['villa'],:],(1,20)),np.reshape(embeddings[t.word_index['elmhurst'],:],(1,20)))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08750361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.19200744551648508"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1. - cdist(np.reshape(embeddings[t.word_index['60126'],:],(1,20)), np.reshape(embeddings[t.word_index['65402'],:],(1,20)), 'cosine')[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4402bd2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c6c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = pd.DataFrame(pd.unique(data.statecode),columns=['states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e086956a",
   "metadata": {},
   "outputs": [],
   "source": [
    "states_list = []\n",
    "\n",
    "for index, row in states.iterrows():\n",
    "    state = row[0].lower()\n",
    "    states_list.append(pd.concat([pd.Series(state), pd.DataFrame(np.reshape(embeddings[t.word_index[state],:],(1,20)))   ],axis=1))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4881ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = pd.concat(states_list)\n",
    "states = states.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6ef199",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "states.columns = range(states.columns.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11494cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = StandardScaler().fit_transform(states.iloc[:,1:-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64311d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TSNE : Compressing the weights to 3 dimensions to plot the data\n",
    "tsne_model = TSNE(perplexity=40, n_components=3, init='pca', n_iter=2500, random_state=seed)\n",
    "new_values = tsne_model.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b322485",
   "metadata": {},
   "outputs": [],
   "source": [
    "principalDf = pd.DataFrame(data = new_values\n",
    "             , columns = ['component 1', 'component 2', 'component 3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09a2021",
   "metadata": {},
   "outputs": [],
   "source": [
    "#principalDf = pd.DataFrame(data = principalComponents\n",
    "#             , columns = ['principal component 1', 'principal component 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d55113",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf = pd.concat([principalDf, states.iloc[:,0]], axis = 1)\n",
    "finalDf.columns = ['x', 'y', 'z', 'State']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156326f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = t.word_index\n",
    "index_to_word = dict()\n",
    "\n",
    "for key in word_to_index:\n",
    "    index_to_word.update({word_to_index[key] : key })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002db0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_3d(finalDf, x='x', y='y', z='z',\n",
    "              color='State')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c0ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "postalcodes = pd.DataFrame(pd.unique(data.postal_code),columns=['postal_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3c4b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "postalcodes_list = []\n",
    "\n",
    "for index, row in postalcodes.iterrows():\n",
    "    postalcode = row[0].lower()\n",
    "    postalcodes_list.append(pd.concat([pd.Series(postalcode), pd.DataFrame(np.reshape(embeddings[t.word_index[postalcode],:],(1,20)))   ],axis=1))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debdbf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "postalcodes = pd.concat(postalcodes_list)\n",
    "postalcodes = postalcodes.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8930b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "postalcodes.columns = range(postalcodes.columns.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a4d2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = StandardScaler().fit_transform(postalcodes.iloc[:,1:-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e534af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TSNE : Compressing the weights to 3 dimensions to plot the data\n",
    "tsne_model = TSNE(perplexity=40, n_components=3, init='pca', n_iter=2500, random_state=23)\n",
    "new_values = tsne_model.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc775c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "principalDf = pd.DataFrame(data = new_values\n",
    "             , columns = ['component 1', 'component 2', 'component 3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519b10bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "principalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b50391",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf = pd.concat([principalDf, postalcodes.iloc[:,0]], axis = 1)\n",
    "finalDf.columns = ['x', 'y', 'z', 'postalcode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764a2085",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf = finalDf[finalDf['postalcode'].str.slice(start=0, stop=3) == '601']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2112a1bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_3d(finalDf, x='x', y='y', z='z',\n",
    "              color='postalcode')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e57d5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_seq = to_categorical([label[0:32]], num_classes=vocab_size)[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb9162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fa08c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
